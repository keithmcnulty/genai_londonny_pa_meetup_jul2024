{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Operating a RAG pipeline using locally hosted small language models\n",
    "\n",
    "A small language model (SLM) in this case is defined as a language model that is less than 10B parameters.  Such models usually take up about 5GB of disk space and can be executed locally on any reasonable spec machine.\n",
    "\n",
    "In this notebook, we set up the following RAG architecture with the NY Times comment set using a local ChromaDB vector database to handle the vector embeddings of the comment set, and using a few options for SLMs involved using ollama. \n",
    "\n",
    "![Simple RAG Architecture](../rag_architecture.jpg) \n",
    "\n",
    "Note that to use ollama you need to install it on your machine.  See [the ollama website](https://ollama.com/) for details.  This workflow also assumes that you have already created the Chroma vector database (see the `chromadb_prep` folder for instructions)."
   ],
   "id": "ce36e490cf908ffe"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-14T13:11:58.449346Z",
     "start_time": "2024-07-14T13:11:58.442482Z"
    }
   },
   "source": [
    "# packages\n",
    "from langchain_community.llms import Ollama\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "\n",
    "# location of pre-built local chromadb\n",
    "CHROMA_DATA_PATH = \"../chroma_data/\"\n",
    "collection_db=\"article_comments\"\n",
    "\n",
    "# chroma client \n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)\n",
    "\n",
    "# SLM options via Ollama\n",
    "mistral = Ollama(model=\"mistral\")\n",
    "llama3 = Ollama(model=\"llama3:70b\")\n",
    "gemma = Ollama(model=\"gemma\")\n",
    "\n",
    "# helper function for prompt construction\n",
    "def construct_prompt(docs: dict, question: str) -> str:\n",
    "    # convert the docs into a numbered list of comments\n",
    "    results_df = pd.DataFrame(docs['documents']).transpose()\n",
    "    results_df.columns = ['Comment']\n",
    "    results_df['ComNum'] = [str(i) for i in range(1, len(results_df) + 1)]\n",
    "    results_df['Numbered Comments'] = results_df['ComNum'] + '. ' + results_df['Comment']\n",
    "\n",
    "    # Collect the results in a context\n",
    "    context = \"\\n\".join([r for r in results_df['Numbered Comments']])\n",
    "\n",
    "    # construct prompt\n",
    "    prompt = f\"\"\"\n",
    "        Answer the following question: {question}.  \n",
    "        Refer only to the following numbered list of comments from NY Times readers when answering: {context}.\n",
    "        Check each numbered comment very carefully and ignore it if it does not contain language that is a close match to the original question.\n",
    "        Provide as much information as possible in the summary, subject to the conditions already given.\n",
    "        Begin your answer with 'Based on the responses from selected NY Times readers', and try to give a sense of majority and minority opinions on the topic, but only if there is an identifiable majority opinion.\n",
    "        If there is not enough information provided to give a summarized opinion, indicate that this is the case.\n",
    "        \"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# RAG pipeline function\n",
    "def ask_question_local(question:str, llm: Ollama() = llama3, \n",
    "                       collection: chromadb.PersistentClient() = collection_db, n_docs:int = 50, \n",
    "                       filters: dict ={}) -> str:\n",
    "    \n",
    "    # Find close documents in chromadb\n",
    "    collection = chroma_client.get_collection(collection)\n",
    "    results = collection.query(\n",
    "       query_texts=[question],\n",
    "       n_results=n_docs,\n",
    "       where=filters\n",
    "    )\n",
    "\n",
    "    prompt = construct_prompt(results, question)\n",
    "    \n",
    "    # generate response\n",
    "    print(llm.invoke(prompt))"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T13:15:16.065671Z",
     "start_time": "2024-07-14T13:14:03.876356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test function\n",
    "ask_question_local(\"What do readers think about US foreign policy towards North Korea?\", llm = llama3, n_docs = 100)"
   ],
   "id": "20d4e963c2426d6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the responses from selected NY Times readers, many are skeptical about the effectiveness of US foreign policy towards North Korea. A significant number of respondents believe that North Korea's leadership will never willingly give up its nuclear weapons or affiliated programs, citing the regime's existential need for a nuclear capability and its history of belligerence.\n",
      "\n",
      "Some readers suggest that the onus is on the rest of the civilized world to contain North Korea through means such as developing anti-missile systems, squeezing Pyongyang financially, humiliating its leaders with sanctions, and developing cyber capabilities. A few respondents also propose more drastic measures, including assassinating NK scientists associated with the missile program and sending Navy SEALs to find secret tunnels and labs.\n",
      "\n",
      "However, a minority opinion suggests that South Korea appears to be taking a more level-headed approach to negotiations, with some readers praising the country's leadership for being the \"adults\" in the situation. A few respondents also propose a slower and more steady pace of negotiation and engagement, citing the effectiveness of economic sanctions in weakening Kim Jong-un's power.\n",
      "\n",
      "Overall, there is no clear majority opinion on the most effective approach to US foreign policy towards North Korea, but a significant number of respondents express skepticism about the regime's willingness to negotiate in good faith.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d1252e79ef6b2da0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
